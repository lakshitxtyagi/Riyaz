{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 ragas: ['Abhogi', 'Ahir Bhairav', 'Bageshree', 'Bhairavi', 'Bhoopali', 'Jog', 'Malhar', 'Shree', 'Todi', 'Yaman']\n",
      "Finding files for raga: Ahir Bhairav\n",
      "Finding files for raga: Malhar\n",
      "Finding files for raga: Bageshree\n",
      "Finding files for raga: Jog\n",
      "Finding files for raga: Yaman\n",
      "Finding files for raga: Bhoopali\n",
      "Finding files for raga: Bhairavi\n",
      "Finding files for raga: Todi\n",
      "Finding files for raga: Shree\n",
      "Finding files for raga: Abhogi\n",
      "Processing 4144 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=479'>480</a>\u001b[0m         \u001b[39mprint\u001b[39m(e)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=481'>482</a>\u001b[0m \u001b[39m# Run the training process\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=482'>483</a>\u001b[0m model, label_map \u001b[39m=\u001b[39m train_and_evaluate_lstm()\n",
      "\u001b[1;32m/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=283'>284</a>\u001b[0m \u001b[39m# Load training data\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=284'>285</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=285'>286</a>\u001b[0m X_train_sequences, y_train, label_map \u001b[39m=\u001b[39m load_training_data_lstm(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=286'>287</a>\u001b[0m     TRAIN_DATA_PATH, cache_dir)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=288'>289</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mData loading completed in \u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m.\u001b[39mtime()\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mstart_time\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=289'>290</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining sequences shape: \u001b[39m\u001b[39m{\u001b[39;00mX_train_sequences\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=173'>174</a>\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=174'>175</a>\u001b[0m \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m audio_paths:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=175'>176</a>\u001b[0m     result \u001b[39m=\u001b[39m process_audio_file_lstm(path, cache_dir)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=176'>177</a>\u001b[0m     results\u001b[39m.\u001b[39mappend(result)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=178'>179</a>\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "\u001b[1;32m/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mpad(y, (\u001b[39m0\u001b[39m, DURATION \u001b[39m*\u001b[39m SAMPLE_RATE \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(y)), \u001b[39m'\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m \u001b[39m# Extract sequential features\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m features \u001b[39m=\u001b[39m extract_sequential_features(y, sr)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m \u001b[39m# Create overlapping windows\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m sequences \u001b[39m=\u001b[39m create_windowed_sequences(features)\n",
      "\u001b[1;32m/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb Cell 1\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m frame_features\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mmean(centroid))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# 4. Spectral Contrast (difference between peaks and valleys)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m contrast \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39mfeature\u001b[39m.\u001b[39mspectral_contrast(y\u001b[39m=\u001b[39mframe, sr\u001b[39m=\u001b[39msr)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m frame_features\u001b[39m.\u001b[39mextend(np\u001b[39m.\u001b[39mmean(contrast, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lakshittyagi/Desktop/Riyaz/LSTM/lstm_resampling.ipynb#W0sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m# 5. Spectral Rolloff (frequency below which most energy exists)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/librosa/feature/spectral.py:515\u001b[0m, in \u001b[0;36mspectral_contrast\u001b[0;34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, freq, fmin, n_bands, quantile, linear)\u001b[0m\n\u001b[1;32m    512\u001b[0m     sub_band \u001b[39m=\u001b[39m sub_band[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m    514\u001b[0m \u001b[39m# Always take at least one bin from each side\u001b[39;00m\n\u001b[0;32m--> 515\u001b[0m idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrint(quantile \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum(current_band))\n\u001b[1;32m    516\u001b[0m idx \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mmaximum(idx, \u001b[39m1\u001b[39m))\n\u001b[1;32m    518\u001b[0m sortedr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msort(sub_band, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import pywt\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "# Define paths and constants\n",
    "TRAIN_DATA_PATH = '../dataset/'  # folder containing raga folders\n",
    "TEST_DATA_PATH = '../Test/'  # folder containing test files\n",
    "SAMPLE_RATE = 22050  # standard sample rate\n",
    "DURATION = 20  # seconds per audio file\n",
    "HOP_LENGTH = 512  # hop length for feature extraction\n",
    "FRAME_SIZE = 2048  # frame size for feature extraction\n",
    "WINDOW_SIZE = 80  # number of frames per window for LSTM input\n",
    "HOP_FRAMES = 40  # hop size between windows (50% overlap)\n",
    "N_MFCC = 13  # number of MFCC coefficients\n",
    "BATCH_SIZE = 32  # batch size for training\n",
    "MAX_EPOCHS = 50  # maximum number of epochs\n",
    "USE_MIXED_PRECISION = True  # Enable mixed precision training\n",
    "CACHE_FEATURES = True  # Cache extracted features to disk\n",
    "FEATURES_CACHE_DIR = './features_cache_lstm/'  # Directory to cache features\n",
    "\n",
    "# Function definitions must be at the module level for proper pickling\n",
    "def extract_sequential_features(y, sr):\n",
    "    \"\"\"Extract sequential features for LSTM processing\"\"\"\n",
    "    # Initialize features array to store per-frame features\n",
    "    features = []\n",
    "    \n",
    "    # Frame the audio signal\n",
    "    for i in range(0, len(y) - FRAME_SIZE, HOP_LENGTH):\n",
    "        frame = y[i:i + FRAME_SIZE]\n",
    "        if len(frame) < FRAME_SIZE:\n",
    "            # Pad the last frame if needed\n",
    "            frame = np.pad(frame, (0, FRAME_SIZE - len(frame)), 'constant')\n",
    "        \n",
    "        # Extract features for this frame\n",
    "        frame_features = []\n",
    "        \n",
    "        # 1. MFCC (tonal characteristics)\n",
    "        mfcc = librosa.feature.mfcc(y=frame, sr=sr, n_mfcc=N_MFCC)\n",
    "        frame_features.extend(np.mean(mfcc, axis=1))\n",
    "        \n",
    "        # 2. Chroma (pitch class distribution)\n",
    "        chroma = librosa.feature.chroma_stft(y=frame, sr=sr)\n",
    "        frame_features.extend(np.mean(chroma, axis=1))\n",
    "        \n",
    "        # 3. Spectral Centroid (brightness of sound)\n",
    "        centroid = librosa.feature.spectral_centroid(y=frame, sr=sr)\n",
    "        frame_features.append(np.mean(centroid))\n",
    "        \n",
    "        # 4. Spectral Contrast (difference between peaks and valleys)\n",
    "        contrast = librosa.feature.spectral_contrast(y=frame, sr=sr)\n",
    "        frame_features.extend(np.mean(contrast, axis=1))\n",
    "        \n",
    "        # 5. Spectral Rolloff (frequency below which most energy exists)\n",
    "        rolloff = librosa.feature.spectral_rolloff(y=frame, sr=sr)\n",
    "        frame_features.append(np.mean(rolloff))\n",
    "        \n",
    "        # 6. Zero Crossing Rate (rhythmic feature)\n",
    "        zcr = librosa.feature.zero_crossing_rate(frame)\n",
    "        frame_features.append(np.mean(zcr))\n",
    "        \n",
    "        features.append(frame_features)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    features = np.array(features)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def create_windowed_sequences(features, window_size=WINDOW_SIZE, hop_size=HOP_FRAMES):\n",
    "    \"\"\"Create overlapping windows for sequential processing\"\"\"\n",
    "    sequences = []\n",
    "    \n",
    "    # Create windows with overlap\n",
    "    for i in range(0, len(features) - window_size + 1, hop_size):\n",
    "        window = features[i:i + window_size]\n",
    "        if len(window) == window_size:  # Ensure we have complete windows\n",
    "            sequences.append(window)\n",
    "    \n",
    "    # If we have at least one sequence, return as numpy array\n",
    "    if sequences:\n",
    "        return np.array(sequences)\n",
    "    else:\n",
    "        # Return at least one sequence (with zero padding if needed)\n",
    "        if len(features) > 0:\n",
    "            padding = np.zeros((window_size - len(features), features.shape[1]))\n",
    "            padded_features = np.vstack([features, padding])\n",
    "            return np.array([padded_features[:window_size]])\n",
    "        else:\n",
    "            # If no features at all, return empty window\n",
    "            return np.zeros((1, window_size, 1))\n",
    "\n",
    "def process_audio_file_lstm(audio_path, cache_dir=None):\n",
    "    \"\"\"Process a single audio file for LSTM with caching option\"\"\"\n",
    "    # Check if cached features exist\n",
    "    if cache_dir is not None:\n",
    "        cache_filename = os.path.basename(audio_path).replace('.', '_') + '_lstm.npz'\n",
    "        cache_path = os.path.join(cache_dir, cache_filename)\n",
    "        \n",
    "        if os.path.exists(cache_path):\n",
    "            cached_data = np.load(cache_path, allow_pickle=True)\n",
    "            return cached_data['sequences']\n",
    "    \n",
    "    try:\n",
    "        # Load audio file with resampling\n",
    "        y, sr = librosa.load(audio_path, sr=SAMPLE_RATE, duration=DURATION, res_type='kaiser_fast')\n",
    "        \n",
    "        # Add small silence if audio is shorter than expected duration\n",
    "        if len(y) < DURATION * SAMPLE_RATE:\n",
    "            y = np.pad(y, (0, DURATION * SAMPLE_RATE - len(y)), 'constant')\n",
    "        \n",
    "        # Extract sequential features\n",
    "        features = extract_sequential_features(y, sr)\n",
    "        \n",
    "        # Create overlapping windows\n",
    "        sequences = create_windowed_sequences(features)\n",
    "        \n",
    "        # Cache features if requested\n",
    "        if cache_dir is not None:\n",
    "            np.savez_compressed(cache_path, sequences=sequences)\n",
    "        \n",
    "        return sequences\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_training_data_lstm(data_path, cache_dir=None):\n",
    "    \"\"\"Load training data for LSTM with parallel processing\"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    label_map = {}\n",
    "    \n",
    "    # Get all raga folders\n",
    "    raga_folders = [f for f in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, f))]\n",
    "    \n",
    "    # Create label mapping\n",
    "    for i, raga in enumerate(sorted(raga_folders)):\n",
    "        label_map[raga] = i\n",
    "    \n",
    "    print(f\"Found {len(raga_folders)} ragas: {sorted(raga_folders)}\")\n",
    "    \n",
    "    # Prepare list of all audio files with their labels\n",
    "    audio_paths = []\n",
    "    audio_labels = []\n",
    "    \n",
    "    for raga in raga_folders:\n",
    "        raga_path = os.path.join(data_path, raga)\n",
    "        print(f\"Finding files for raga: {raga}\")\n",
    "        \n",
    "        # Get all audio files\n",
    "        audio_files = [f for f in os.listdir(raga_path) if f.endswith(('.wav', '.mp3'))]\n",
    "        \n",
    "        for audio_file in audio_files:\n",
    "            audio_path = os.path.join(raga_path, audio_file)\n",
    "            audio_paths.append(audio_path)\n",
    "            audio_labels.append(label_map[raga])\n",
    "    \n",
    "    # Process files sequentially instead of using multiprocessing\n",
    "    print(f\"Processing {len(audio_paths)} files...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Sequential processing - safe alternative\n",
    "    results = []\n",
    "    for path in audio_paths:\n",
    "        result = process_audio_file_lstm(path, cache_dir)\n",
    "        results.append(result)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Feature extraction completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Collect results\n",
    "    for i, seq in enumerate(results):\n",
    "        if seq is not None:\n",
    "            # For each sequence window from this audio file\n",
    "            for window in seq:\n",
    "                sequences.append(window)\n",
    "                labels.append(audio_labels[i])\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    sequences = np.array(sequences)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    print(f\"Created {len(sequences)} sequence windows from {len(audio_paths)} audio files\")\n",
    "    print(f\"Sequence shape: {sequences.shape}\")\n",
    "    \n",
    "    return sequences, labels, label_map\n",
    "\n",
    "def load_test_data_lstm(test_path, label_map, cache_dir=None):\n",
    "    \"\"\"Load test data for LSTM model\"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    file_sequences = []  # To track which sequences belong to which file\n",
    "    file_names = []\n",
    "    \n",
    "    # Get all test files\n",
    "    test_files = [f for f in os.listdir(test_path) if f.endswith(('.wav', '.mp3'))]\n",
    "    audio_paths = []\n",
    "    expected_labels = []\n",
    "    \n",
    "    for file in test_files:\n",
    "        # Extract raga name from filename (assuming format is raga_name_test_XX.wav)\n",
    "        raga_name = file.split('_')[0]\n",
    "        \n",
    "        if raga_name in label_map:\n",
    "            audio_path = os.path.join(test_path, file)\n",
    "            audio_paths.append(audio_path)\n",
    "            expected_labels.append(label_map[raga_name])\n",
    "            file_names.append(file)\n",
    "    \n",
    "    # Process files sequentially instead of using multiprocessing\n",
    "    results = []\n",
    "    for path in audio_paths:\n",
    "        result = process_audio_file_lstm(path, cache_dir)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Collect results\n",
    "    for i, seq in enumerate(results):\n",
    "        if seq is not None:\n",
    "            # For testing, keep track of which sequences belong to which file\n",
    "            file_sequences.append(len(sequences))  # Store the start index\n",
    "            \n",
    "            # For each sequence window from this audio file\n",
    "            for window in seq:\n",
    "                sequences.append(window)\n",
    "                labels.append(expected_labels[i])\n",
    "            \n",
    "            file_sequences.append(len(sequences))  # Store the end index\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    sequences = np.array(sequences)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return sequences, labels, file_names, file_sequences\n",
    "\n",
    "def build_lstm_model(input_shape, num_classes):\n",
    "    \"\"\"Build LSTM model for sequential audio classification\"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Bidirectional LSTM layers\n",
    "        layers.Bidirectional(layers.LSTM(128, return_sequences=True), \n",
    "                           input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Bidirectional(layers.LSTM(64)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Dense layers for classification\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_lstm():\n",
    "    \"\"\"Train and evaluate LSTM model for raga classification\"\"\"\n",
    "    cache_dir = FEATURES_CACHE_DIR if CACHE_FEATURES else None\n",
    "    \n",
    "    # Load training data\n",
    "    start_time = time.time()\n",
    "    X_train_sequences, y_train, label_map = load_training_data_lstm(\n",
    "        TRAIN_DATA_PATH, cache_dir)\n",
    "    \n",
    "    print(f\"Data loading completed in {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Training sequences shape: {X_train_sequences.shape}\")\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_sequences, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Validation set shape: {X_val.shape}\")\n",
    "    \n",
    "    # Build and compile the LSTM model\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])  # (time_steps, features)\n",
    "    model = build_lstm_model(input_shape, num_classes=len(label_map))\n",
    "    model.summary()\n",
    "    \n",
    "    # Create TF datasets for better performance\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    train_ds = train_ds.cache().shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "    val_ds = val_ds.cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Create callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            patience=8, restore_best_weights=True, monitor='val_accuracy'),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            factor=0.5, patience=4, min_lr=0.00001, monitor='val_loss'),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            'best_raga_lstm_model.h5', save_best_only=True, monitor='val_accuracy')\n",
    "    ]\n",
    "    \n",
    "    # Train the model\n",
    "    training_start = time.time()\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=MAX_EPOCHS,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    training_time = time.time() - training_start\n",
    "    print(f\"Model training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "    plt.title('LSTM Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train')\n",
    "    plt.plot(history.history['val_loss'], label='Validation')\n",
    "    plt.title('LSTM Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lstm_training_history.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    X_test_sequences, y_test, file_names, file_sequences = load_test_data_lstm(\n",
    "        TEST_DATA_PATH, label_map, cache_dir)\n",
    "    \n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test_sequences, y_test)).batch(BATCH_SIZE)\n",
    "    \n",
    "    test_loss, test_acc = model.evaluate(test_ds)\n",
    "    print(f\"Test accuracy on sequence level: {test_acc:.4f}\")\n",
    "    \n",
    "    # Get predictions for each sequence\n",
    "    y_pred_seq = model.predict(X_test_sequences)\n",
    "    y_pred_seq_classes = np.argmax(y_pred_seq, axis=1)\n",
    "    \n",
    "    # For each file, aggregate predictions across all its sequences (majority voting)\n",
    "    file_predictions = []\n",
    "    file_true_labels = []\n",
    "    \n",
    "    for i in range(0, len(file_sequences), 2):\n",
    "        start_idx = file_sequences[i]\n",
    "        end_idx = file_sequences[i+1]\n",
    "        \n",
    "        # Get predictions for all sequences in this file\n",
    "        file_seq_preds = y_pred_seq_classes[start_idx:end_idx]\n",
    "        \n",
    "        # Majority voting\n",
    "        if len(file_seq_preds) > 0:\n",
    "            unique_preds, counts = np.unique(file_seq_preds, return_counts=True)\n",
    "            file_pred = unique_preds[np.argmax(counts)]\n",
    "            file_predictions.append(file_pred)\n",
    "            \n",
    "            # The true label is the same for all sequences of a file\n",
    "            file_true_labels.append(y_test[start_idx])\n",
    "    \n",
    "    # Calculate file-level accuracy\n",
    "    file_acc = np.mean(np.array(file_predictions) == np.array(file_true_labels))\n",
    "    print(f\"Test accuracy on file level (majority voting): {file_acc:.4f}\")\n",
    "    \n",
    "    # Inverse mapping for class names\n",
    "    id_to_raga = {v: k for k, v in label_map.items()}\n",
    "    \n",
    "    # Create confusion matrix at file level\n",
    "    cm = confusion_matrix(file_true_labels, file_predictions)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=[id_to_raga[i] for i in range(len(label_map))],\n",
    "               yticklabels=[id_to_raga[i] for i in range(len(label_map))])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix (File Level)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lstm_confusion_matrix.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report at file level\n",
    "    print(\"\\nClassification Report (File Level):\")\n",
    "    class_report = classification_report(\n",
    "        file_true_labels, file_predictions, \n",
    "        target_names=[id_to_raga[i] for i in range(len(label_map))],\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    # Convert to DataFrame for better visualization\n",
    "    df_report = pd.DataFrame(class_report).transpose()\n",
    "    print(df_report)\n",
    "    \n",
    "    # Save file-level predictions to CSV\n",
    "    results_df = pd.DataFrame({\n",
    "        'File': file_names[:len(file_predictions)],\n",
    "        'True Raga': [id_to_raga[i] for i in file_true_labels],\n",
    "        'Predicted Raga': [id_to_raga[i] for i in file_predictions],\n",
    "        'Correct': np.array(file_true_labels) == np.array(file_predictions)\n",
    "    })\n",
    "    results_df.to_csv('lstm_test_predictions.csv', index=False)\n",
    "    print(\"Prediction results saved to 'lstm_test_predictions.csv'\")\n",
    "    \n",
    "    # Save the model\n",
    "    model.save('raga_classification_lstm_model.h5')\n",
    "    print(\"Model saved as 'raga_classification_lstm_model.h5'\")\n",
    "    \n",
    "    # Print performance summary\n",
    "    print(\"\\nPerformance Summary:\")\n",
    "    print(f\"Total data processing and training time: {(time.time() - start_time)/60:.2f} minutes\")\n",
    "    print(f\"Model training time: {training_time/60:.2f} minutes\")\n",
    "    print(f\"Final test accuracy (sequence level): {test_acc:.4f}\")\n",
    "    print(f\"Final test accuracy (file level): {file_acc:.4f}\")\n",
    "    \n",
    "    return model, label_map\n",
    "\n",
    "# Additional utility function to visualize LSTM attention on audio features\n",
    "def visualize_sequence_attention(model, audio_path, label_map):\n",
    "    \"\"\"Visualize which parts of the audio the LSTM pays attention to\"\"\"\n",
    "    # This would require a model with attention mechanism\n",
    "    # Implementation would depend on the specific attention layer used\n",
    "    pass\n",
    "\n",
    "# For safer multiprocessing, wrap the main execution in a proper if __name__ block\n",
    "# and set the correct start method\n",
    "if __name__ == \"__main__\":\n",
    "    # Set multiprocessing start method to 'spawn' for better compatibility\n",
    "    # This is especially important on Windows and macOS\n",
    "    try:\n",
    "        import multiprocessing\n",
    "        multiprocessing.set_start_method('spawn')\n",
    "    except RuntimeError:\n",
    "        # The start method might already be set\n",
    "        pass\n",
    "    \n",
    "    # Enable mixed precision training if GPU is available\n",
    "    if USE_MIXED_PRECISION and tf.config.list_physical_devices('GPU'):\n",
    "        policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "        tf.keras.mixed_precision.set_global_policy(policy)\n",
    "        print(\"Mixed precision training enabled\")\n",
    "    \n",
    "    # Create cache directory if it doesn't exist\n",
    "    if CACHE_FEATURES and not os.path.exists(FEATURES_CACHE_DIR):\n",
    "        os.makedirs(FEATURES_CACHE_DIR)\n",
    "    \n",
    "    # Set memory growth for GPUs to avoid OOM errors\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\"GPU(s) detected: {len(gpus)}\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Run the training process\n",
    "    model, label_map = train_and_evaluate_lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
